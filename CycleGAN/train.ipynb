{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using CUDA\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m\n\u001b[1;32m     72\u001b[0m transforms_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     73\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((hp\u001b[38;5;241m.\u001b[39mimg_size, hp\u001b[38;5;241m.\u001b[39mimg_size), Image\u001b[38;5;241m.\u001b[39mBICUBIC),\n\u001b[1;32m     74\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     75\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)),\n\u001b[1;32m     76\u001b[0m ]\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_train_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     86\u001b[0m     ImageDataset(root_path, mode\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mdataset_test_mode, transforms_\u001b[38;5;241m=\u001b[39mtransforms_),\n\u001b[1;32m     87\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     88\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     89\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     90\u001b[0m )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Function to save image samples\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import datetime\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from matplotlib.pyplot import figure\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from utils import *\n",
    "from cyclegan import *\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"Using CUDA\" if cuda else \"Not using CUDA\")\n",
    "\n",
    "# Set Tensor type based on CUDA availability\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# Define hyperparameters\n",
    "class Hyperparameters:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "hp = Hyperparameters(\n",
    "    epoch=0,\n",
    "    n_epochs=200,\n",
    "    dataset_train_mode=\"train\",\n",
    "    dataset_test_mode=\"test\",\n",
    "    batch_size=4,\n",
    "    lr=0.0002,\n",
    "    decay_start_epoch=100,\n",
    "    b1=0.5,\n",
    "    b2=0.999,\n",
    "    n_cpu=8,\n",
    "    img_size=128,\n",
    "    channels=3,\n",
    "    n_critic=5,\n",
    "    sample_interval=100,\n",
    "    num_residual_blocks=99,\n",
    "    lambda_cyc=10.0,\n",
    "    lambda_id=5.0,\n",
    ")\n",
    "\n",
    "# Root path for datasets\n",
    "root_path = \"/content/drive/MyDrive/All_Datasets/summer2winter_yosemite\"\n",
    "\n",
    "# Define image visualization methods\n",
    "def show_img(img, size=10):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0) * 2, hp.channels, hp.img_size, hp.img_size)\n",
    "    return x\n",
    "\n",
    "def plot_output(path, x, y):\n",
    "    img = mpimg.imread(path)\n",
    "    plt.figure(figsize=(x, y))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "# Define image transforms\n",
    "transforms_ = [\n",
    "    transforms.Resize((hp.img_size, hp.img_size), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "# Load datasets\n",
    "train_dataloader = DataLoader(\n",
    "    ImageDataset(root_path, mode=hp.dataset_train_mode, transforms_=transforms_),\n",
    "    batch_size=hp.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(root_path, mode=hp.dataset_test_mode, transforms_=transforms_),\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "# Function to save image samples\n",
    "def save_img_samples(batches_done):\n",
    "    print(\"batches_done \", batches_done)\n",
    "    imgs = next(iter(val_dataloader))\n",
    "\n",
    "    Gen_AB.eval()\n",
    "    Gen_BA.eval()\n",
    "\n",
    "    real_A = imgs[\"A\"].type(Tensor)\n",
    "    fake_B = Gen_AB(real_A)\n",
    "    real_B = imgs[\"B\"].type(Tensor)\n",
    "    fake_A = Gen_BA(real_B)\n",
    "\n",
    "    real_A = make_grid(real_A, nrow=16, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=16, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=16, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=16, normalize=True)\n",
    "\n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    path = root_path + \"/%s.png\" % (batches_done)\n",
    "    save_image(image_grid, path, normalize=False)\n",
    "    return path\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "input_shape = (hp.channels, hp.img_size, hp.img_size)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "Gen_AB = GeneratorResNet(input_shape, hp.num_residual_blocks)\n",
    "Gen_BA = GeneratorResNet(input_shape, hp.num_residual_blocks)\n",
    "Disc_A = Discriminator(input_shape)\n",
    "Disc_B = Discriminator(input_shape)\n",
    "\n",
    "if cuda:\n",
    "    Gen_AB = Gen_AB.cuda()\n",
    "    Gen_BA = Gen_BA.cuda()\n",
    "    Disc_A = Disc_A.cuda()\n",
    "    Disc_B = Disc_B.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_cycle.cuda()\n",
    "    criterion_identity.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "Gen_AB.apply(initialize_conv_weights_normal)\n",
    "Gen_BA.apply(initialize_conv_weights_normal)\n",
    "Disc_A.apply(initialize_conv_weights_normal)\n",
    "Disc_B.apply(initialize_conv_weights_normal)\n",
    "\n",
    "# Buffers for previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(Gen_AB.parameters(), Gen_BA.parameters()),\n",
    "    lr=hp.lr,\n",
    "    betas=(hp.b1, hp.b2),\n",
    ")\n",
    "optimizer_Disc_A = torch.optim.Adam(Disc_A.parameters(), lr=hp.lr, betas=(hp.b1, hp.b2))\n",
    "optimizer_Disc_B = torch.optim.Adam(Disc_B.parameters(), lr=hp.lr, betas=(hp.b1, hp.b2))\n",
    "\n",
    "# Learning rate schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step\n",
    ")\n",
    "lr_scheduler_Disc_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_Disc_A, lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step\n",
    ")\n",
    "lr_scheduler_Disc_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_Disc_B, lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step\n",
    ")\n",
    "\n",
    "# Training function\n",
    "def train(\n",
    "    Gen_BA,\n",
    "    Gen_AB,\n",
    "    Disc_A,\n",
    "    Disc_B,\n",
    "    train_dataloader,\n",
    "    n_epochs,\n",
    "    criterion_identity, \n",
    "    criterion_cycle,\n",
    "    lambda_cyc,\n",
    "    criterion_GAN,\n",
    "    optimizer_G,\n",
    "    fake_A_buffer,\n",
    "    fake_B_buffer,\n",
    "    clear_output,\n",
    "    optimizer_Disc_A,\n",
    "    optimizer_Disc_B,\n",
    "    Tensor,\n",
    "    sample_interval,\n",
    "    lambda_id,\n",
    "):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(hp.epoch, n_epochs):\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            real_A = batch[\"A\"].type(Tensor)\n",
    "            real_B = batch[\"B\"].type(Tensor)\n",
    "\n",
    "            valid = torch.ones((real_A.size(0), *Disc_A.output_shape), requires_grad=False).type(Tensor)\n",
    "            fake = torch.zeros((real_A.size(0), *Disc_A.output_shape), requires_grad=False).type(Tensor)\n",
    "\n",
    "            Gen_AB.train()  # Horses are converted to Zebras(Fake)\n",
    "            Gen_BA.train()  # Zebras are converted to Horses(Fake)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Identity Loss \n",
    "            # First pass real A images to the Generator, that will generate A-domain images\n",
    "            loss_id_A = criterion_identity(Gen_BA(real_A), real_A)\n",
    "            # Then pass real B images to the Generator, that will generate B-domain images\n",
    "            loss_id_B = criterion_identity(Gen_AB(real_B), real_B)\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "            # GAN losses for GAN_AB\n",
    "            fake_B = Gen_AB(real_A)   # Gen_AB is generating fake B from real A images (horse to zebra)\n",
    "            loss_GAN_AB = criterion_GAN(Disc_B(fake_B), valid)\n",
    "            \n",
    "            # GAN losses for GAN_BA  \n",
    "            fake_A = Gen_BA(real_B)  # Gen_BA is generating fake A from real B images (zebra to horse)\n",
    "            loss_GAN_BA = criterion_GAN(Disc_A(fake_A), valid)\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "            \n",
    "            # Cycle Consistency Loss\n",
    "            reconstructed_A = Gen_BA(fake_B) # Here the fake zebra is being converted back to horse using Gen_BA\n",
    "            loss_cycle_A = criterion_cycle(reconstructed_A, real_A) # Forward loss\n",
    "            reconstructed_B = Gen_AB(fake_A) # Here the fake horse is being converted back zebra using Gen_AB\n",
    "            loss_cycle_B = criterion_cycle(reconstructed_B, real_B) # Backward loss\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            optimizer_Disc_A.zero_grad()\n",
    "            loss_real = criterion_GAN(Disc_A(real_A), valid)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(Disc_A(fake_A_.detach()), fake)\n",
    "            loss_Disc_A = (loss_real + loss_fake) / 2\n",
    "            loss_Disc_A.backward()\n",
    "            optimizer_Disc_A.step()\n",
    "\n",
    "            optimizer_Disc_B.zero_grad()\n",
    "            loss_real = criterion_GAN(Disc_B(real_B), valid)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(Disc_B(fake_B_.detach()), fake)\n",
    "            loss_Disc_B = (loss_real + loss_fake) / 2\n",
    "            loss_Disc_B.backward()\n",
    "            optimizer_Disc_B.step()\n",
    "\n",
    "            loss_D = (loss_Disc_A + loss_Disc_B) / 2\n",
    "\n",
    "            batches_done = epoch * len(train_dataloader) + i\n",
    "            batches_left = n_epochs * len(train_dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(\n",
    "                seconds=batches_left * (time.time() - prev_time)\n",
    "            )\n",
    "            prev_time = time.time()\n",
    "\n",
    "            print(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    n_epochs,\n",
    "                    i,\n",
    "                    len(train_dataloader),\n",
    "                    loss_D.item(),\n",
    "                    loss_G.item(),\n",
    "                    loss_GAN.item(),\n",
    "                    loss_cycle.item(),\n",
    "                    loss_identity.item(),\n",
    "                    time_left,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if batches_done % sample_interval == 0:\n",
    "                clear_output()\n",
    "                plot_output(save_img_samples(batches_done), 30, 40)\n",
    "\n",
    "# Execute the final training function\n",
    "train(\n",
    "    Gen_BA=Gen_BA,\n",
    "    Gen_AB=Gen_AB,\n",
    "    Disc_A=Disc_A,\n",
    "    Disc_B=Disc_B,\n",
    "    train_dataloader=train_dataloader,\n",
    "    n_epochs=hp.n_epochs,\n",
    "    criterion_identity=criterion_identity,\n",
    "    criterion_cycle=criterion_cycle,\n",
    "    lambda_cyc=hp.lambda_cyc,\n",
    "    criterion_GAN=criterion_GAN,\n",
    "    optimizer_G=optimizer_G,\n",
    "    fake_A_buffer=fake_A_buffer,\n",
    "    fake_B_buffer=fake_B_buffer,\n",
    "    clear_output=clear_output,\n",
    "    optimizer_Disc_A=optimizer_Disc_A,\n",
    "    optimizer_Disc_B=optimizer_Disc_B,\n",
    "    Tensor=Tensor,\n",
    "    sample_interval=hp.sample_interval,\n",
    "    lambda_id=hp.lambda_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
